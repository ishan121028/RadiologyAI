# Minimal CriticalAlert AI Configuration
# Simplest possible setup - just like demo-question-answering/app.yaml

# Data source - incoming radiology PDFs
$sources:
  - !pw.io.fs.read
    path: data/incoming
    format: binary
    with_metadata: true
    mode: streaming

# LandingAI parser (replaces DoclingParser in existing examples)
$parser: !src.parsers.landingai_parser.LandingAIRadiologyParser
  api_key: $LANDINGAI_API_KEY
  cache_strategy: !pw.udfs.DefaultCache {}

# LLM for medical recommendations
$llm: !pw.xpacks.llm.llms.OpenAIChat
  model: "gpt-4o-mini"
  cache_strategy: !pw.udfs.DefaultCache {}
  temperature: 0

# Embedder for medical text
$embedder: !pw.xpacks.llm.embedders.OpenAIEmbedder
  model: "text-embedding-3-small"
  cache_strategy: !pw.udfs.DefaultCache {}

# Text splitter for medical documents
$splitter: !pw.xpacks.llm.splitters.TokenCountSplitter
  max_tokens: 800

# Document store for radiology reports
$document_store: !src.parsers.landingai_parser.RadiologyDocumentStore
  data_sources: $sources
  landingai_api_key: $LANDINGAI_API_KEY
  splitter: $splitter
  cache_strategy: !pw.udfs.DefaultCache {}

# Critical alert answerer (like question_answerer in other examples)
critical_alert_answerer: !src.intelligence.critical_alert_answerer.CriticalAlertQuestionAnswerer
  llm: $llm
  document_store: $document_store

# Server configuration (same as other examples)
host: "0.0.0.0"
port: 8000
with_cache: true
terminate_on_error: false
